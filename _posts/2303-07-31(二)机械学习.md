---
layout: post
title: (二)机械学习_keras
subtitle: 
description: 
header-img: 
date:       2023-7-31 16:12:01
---

# 损失函数
又称目标函数、优化评分函数
```
model.compile(loss='mean_squared_error', optimizer='sgd')
# 或者
from keras import losses
model.compile(loss=losses.mean_squared_error, optimizer='sgd')
```
现有的损失函数名，或者一个 TensorFlow/Theano 符号函数
该符号函数为每个数据点返回一个标量：
-	y_true: 	真是标签，TensorFlow/Theano 张量
- 	y_pred: 	预测值TensorFlow/Theano 张量，其 shape 与 y_true 相同
实际的优化目标是所有数据点的输出数组的平均值。
[更多](https://github.com/keras-team/keras/blob/master/keras/losses.py)

`logcosh`预测误差的双曲余弦的对数
```
logcosh(y_true, y_pred)
```
- 小的 x，log(cosh(x)) 近似等于 (x ** 2) / 2。
- 大的 x，近似于 abs(x) - log(2)。
这表示 'logcosh' 与均方误差大致相同，但是不会受到偶尔疯狂的错误预测的强烈影响。
- y_true: 目标真实值的张量。
- y_pred: 目标预测值的张量。
返回：每个样本都有一个标量损失的张量

备注: 张量[tensor]是一个可用来表示在一些向量、纯量和其他张量之间的线性关系的多线性函数,
在数学里，张量是一种几何实体，或者说广义上的“数量”。张量概念包括纯量、向量和线性算子。张量可以用坐标系统来表达，记作纯量的数组，


# 评价函数
用于评估当前训练模型的性能。当模型编译后（compile），评价函数应该作为 metrics 的参数来输入。
```
model.compile(loss='mean_squared_error',
              optimizer='sgd',
              metrics=['mae', 'acc'])
			  
#  或者
from keras import metrics

model.compile(loss='mean_squared_error',
              optimizer='sgd',
              metrics=[metrics.mae, metrics.categorical_accuracy])
```

评价函数和 损失函数 相似，只不过**评价函数的结果不会用于训练过程中**。

自定义评价函数：
应该在编译的时候（compile）传递进去。该函数需要以 (y_true, y_pred) 作为输入参数，并返回一个张量作为输出结果。
```
import keras.backend as K

def mean_pred(y_true, y_pred):
    return K.mean(y_pred)

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy', mean_pred])
```

# 优化器
优化器 (optimizer) 是编译 Keras 模型的所需的两个参数之一

可以先实例化一个优化器对象，然后将它传入 model.compile()

```
from keras import optimizers

model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('softmax'))

sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)
```

可以通过名称来调用优化器,将使用优化器的默认参数:
```
# 传入优化器名称: 默认参数将被采用
model.compile(loss='mean_squared_error', optimizer='sgd')
```

## 公共参数
参数 clipnorm 和 clipvalue 能在所有的优化器中使用，用于控制梯度裁剪（Gradient Clipping）
```
from keras import optimizers

# 所有参数梯度将被裁剪，让其l2范数最大为1：g * 1 / max(1, l2_norm)
sgd = optimizers.SGD(lr=0.01, clipnorm=1.)


```

```
from keras import optimizers

# 所有参数d 梯度将被裁剪到数值范围内：
# 最大值0.5
# 最小值-0.5
sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)
```

## 随机梯度下降优化器
```
keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)
```
包含扩展功能的支持： - 动量（momentum）优化, - 学习率衰减（每次参数更新后） - Nestrov 动量 (NAG) 优化
参数
	- lr: float >= 0. 学习率。
	- momentum: float >= 0. 参数，用于加速 SGD 在相关方向上前进，并抑制震荡。
	- decay: float >= 0. 每次参数更新后学习率衰减值。
	- nesterov: boolean. 是否使用 Nesterov 动量。